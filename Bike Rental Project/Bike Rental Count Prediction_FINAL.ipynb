{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rental Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries for analysis of data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "os.chdir(\"D:\\Data Science\\Assignments\\Project\")\n",
    "\n",
    "# lets Check working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "Bike_Data = pd.read_csv(\"day.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions(no of rows and no of columns)\n",
    "Bike_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check names of dataset\n",
    "Bike_Data.columns\n",
    "\n",
    "# Rename variables in dataset\n",
    "Bike_Data = Bike_Data.rename(columns = {'instant':'index','dteday':'date','yr':'year','mnth':'month','weathersit':'weather',\n",
    "                                        'temp':'temperature','hum':'humidity','cnt':'count'})\n",
    "\n",
    "Bike_Data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see first five observations of our data\n",
    "Bike_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see last five observations of our data\n",
    "Bike_Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the datatypes of the given data\n",
    "Bike_Data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets Check summary of the dataset \n",
    "Bike_Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Identification \n",
    "Bike_Data['count'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets drop some variables because it doesnot carry any useful information\n",
    "\n",
    "Bike_Data = Bike_Data.drop(['casual','registered','index','date'],axis=1)\n",
    "\n",
    "# Lets check dimensions of data after removing some variables\n",
    "Bike_Data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continous Variables \n",
    "cnames= ['temperature', 'atemp', 'humidity', 'windspeed', 'count']\n",
    "\n",
    "# Categorical variables-\n",
    "cat_cnames=['season', 'year', 'month', 'holiday', 'weekday', 'workingday','weather']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA or Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value anlysis\n",
    "\n",
    "# to check if there is any missing values\n",
    "Missing_val = Bike_Data.isnull().sum()\n",
    "Missing_val\n",
    "# In our dataset we dont have any missing values.so that we dont need to do any imputation methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Analysis\n",
    "\n",
    "# Lets save copy of dataset before preprocessing\n",
    "df = Bike_Data.copy()\n",
    "Bike_Data = df.copy() \n",
    "\n",
    "# Using seaborn library, we can viualize the outliers by plotting box plot\n",
    "for i in cnames:\n",
    "    print(i)\n",
    "    sns.boxplot(y=Bike_Data[i])\n",
    "    plt.xlabel(i)\n",
    "    plt.ylabel(\"values\")\n",
    "    plt.title(\"Boxplot of \"+i)\n",
    "    plt.show()\n",
    "    \n",
    "# From boxplot we can see inliers in humidity and outliers in windspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets detect and remove outliers\n",
    "for i in cnames:\n",
    "    print(i)\n",
    "    # Quartiles and IQR\n",
    "    q25,q75 = np.percentile(Bike_Data[i],[25,75])\n",
    "    IQR = q75-q25\n",
    "    \n",
    "    # Lower and upper limits \n",
    "    Minimum = q25 - (1.5 * IQR)\n",
    "    print(Minimum)\n",
    "    Maximum = q75 + (1.5 * IQR)\n",
    "    print(Maximum)\n",
    "    \n",
    "    Minimum = Bike_Data.loc[Bike_Data[i] < Minimum ,i] \n",
    "    Maximum = Bike_Data.loc[Bike_Data[i] > Maximum ,i]\n",
    "\n",
    "#we substituted minimum values for inliers and maximum values for outliers.\n",
    "#from that we removed all the outliers.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after replacing the outliers,let us plot boxplot for understanding\n",
    "for i in cnames:\n",
    "    print(i)\n",
    "    sns.boxplot(y=Bike_Data[i])\n",
    "    plt.xlabel(i)\n",
    "    plt.ylabel(\"values\")\n",
    "    plt.title(\"Boxplot of \"+i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis \n",
    "\n",
    "# temperature \n",
    "sns.FacetGrid(Bike_Data , height = 5).map(sns.distplot,'temperature').add_legend()\n",
    "#normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# humidity \n",
    "sns.FacetGrid(Bike_Data , height = 5).map(sns.distplot,'humidity').add_legend()\n",
    "#normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windspeed\n",
    "sns.FacetGrid(Bike_Data , height = 5).map(sns.distplot,'windspeed').add_legend()\n",
    "#normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atemp\n",
    "sns.FacetGrid(Bike_Data , height = 5).map(sns.distplot,'atemp').add_legend()\n",
    "#normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "sns.FacetGrid(Bike_Data , height = 5).map(sns.distplot,'count').add_legend()\n",
    "#normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis ------------------------------------------------------\n",
    "# Lets check impact of continous variables on target variable\n",
    "\n",
    "# count vs temperature\n",
    "sns.violinplot(x='count',y='temperature',data=Bike_Data)\n",
    "\n",
    "#temperature is directly proportional to each other\n",
    "#as temperature increases bike rental count also increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vs humidity\n",
    "\n",
    "sns.violinplot(x='count',y='humidity',data=Bike_Data)\n",
    "\n",
    "# Apart from humidity,Bike rental count does not get affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vs windspeed\n",
    "\n",
    "sns.violinplot(x='count',y='windspeed',data=Bike_Data)\n",
    "\n",
    "# Apart from windspeed, Bike rental count does not get affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for categorical variables\n",
    "\n",
    "\n",
    "# SEASON\n",
    "print(Bike_Data.groupby(['season'])['count'].sum())\n",
    "#based on the season, bike rental count is high in season 3 which is fall and low in season 1 which is spring\n",
    "\n",
    "#lets visualize the count using scatterplot\n",
    "sns.scatterplot(x='season',y='count',data = Bike_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YEAR\n",
    "print(Bike_Data.groupby(['year'])['count'].sum())\n",
    "#based on the year, bike rental count is high in the year 1 which is 2012\n",
    "\n",
    "#lets visualize the count using scatterplot\n",
    "sns.scatterplot(x='year',y='count',data = Bike_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONTH\n",
    "print(Bike_Data.groupby(['month'])['count'].sum())\n",
    "#Based on the month, Bike rental count is high in 8 which is in august and low in 1 which is in january\n",
    "\n",
    "#lets visualize the count using scatterplot\n",
    "sns.scatterplot(x='month',y='count',data = Bike_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOLIDAY\n",
    "print(Bike_Data.groupby(['holiday'])['count'].sum())\n",
    "#Based on the holiday, bike rental count is high in 0 which is holiday and low in 1 which is working day\n",
    "\n",
    "#lets visualize the count using scatterplot\n",
    "sns.scatterplot(x='holiday',y='count',data = Bike_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAKDAY\n",
    "print(Bike_Data.groupby(['weekday'])['count'].sum())\n",
    "#Based on the weakday, bike rental count is high in 5 which is friday and low in 0 which is sunday\n",
    "\n",
    "#lets visualize the count using scatterplot\n",
    "sns.scatterplot(x='weekday',y='count',data = Bike_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKINGDAY\n",
    "print(Bike_Data.groupby(['workingday'])['count'].sum())\n",
    "#Based on the workingday, Bike rental count is high in 1 which is working day and low in 0 which is hoiday\n",
    "\n",
    "#lets visualize the count using scatterplot\n",
    "sns.scatterplot(x='workingday',y='count',data = Bike_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEATHER\n",
    "print(Bike_Data.groupby(['weather'])['count'].sum())\n",
    "#Based n the weather bike rental count is higher in 1 which clear,few clouds,partly cloudy and there is no bikes rental in 4\n",
    "\n",
    "#lets visualize the count using scatterplot\n",
    "sns.scatterplot(x='weather',y='count',data = Bike_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bike rented with respected to tempeature and humidity\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.scatterplot(x=\"temperature\", y=\"count\",\n",
    "                hue=\"humidity\", size=\"count\",\n",
    "                palette=\"rainbow\",sizes=(1, 100), linewidth=0,\n",
    "                data=Bike_Data,ax=ax)\n",
    "plt.title(\"Varation in bike rented with respect to temperature and humidity\")\n",
    "plt.ylabel(\"Bike rental count\")\n",
    "plt.xlabel(\"temperature\")\n",
    "\n",
    "# based on the below plot we know that bike rental is higher when the \n",
    "                            #temperature is between 0.4 to 0.8 \n",
    "                            #humidity less than 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bikes rented with respect to temperature and windspeed\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "sns.scatterplot(x=\"temperature\", y=\"count\",\n",
    "                hue=\"windspeed\", size=\"humidity\",\n",
    "                palette=\"rainbow\",sizes=(1, 100), linewidth=0,\n",
    "                data=Bike_Data,ax=ax)\n",
    "plt.title(\"Varation in bike rented with respect to  temperature and windspeed\")\n",
    "plt.ylabel(\"Bike rental count\")\n",
    "plt.xlabel(\"temperature\")\n",
    "\n",
    "#based on the below plot we know that bike rental is higher when the \n",
    "                            #temperature is between 0.4 to 0.8 \n",
    "                            #humidity is less than 0.8\n",
    "                            #windspeed is less than 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bikes rented with respect to temperature and season\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "sns.scatterplot(x=\"temperature\", y=\"count\",\n",
    "                hue=\"season\", size=\"count\",style= \"weather\",\n",
    "                palette=\"rainbow\",sizes=(1, 100), linewidth=0,\n",
    "                data=Bike_Data,ax=ax)\n",
    "plt.title(\"Varation in bike rented with respect to temperature and season\")\n",
    "plt.ylabel(\"Bike rental count\")\n",
    "plt.xlabel(\"Normalized temperature\")\n",
    "\n",
    "#based on the below plot we know that bike rental is higher when the \n",
    "                            #temperature is between 0.4 to 0.8 \n",
    "                            #season was 2 and 3\n",
    "                            #weather was from 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets save dataset after outlier analysis \n",
    "df =  Bike_Data.copy()\n",
    "Bike_Data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "\n",
    "# Correlation matrix continuous variables\n",
    "Bike_corr= Bike_Data.loc[:,cnames]\n",
    "\n",
    "# Generate correlation matrix\n",
    "corr_matrix = Bike_corr.corr()\n",
    "(print(corr_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the width and hieght of the plot\n",
    "f, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "#Plot using seaborn library\n",
    "sns.heatmap(corr_matrix, mask=np.zeros_like(corr_matrix, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax,annot=True)\n",
    "\n",
    "plt.title(\"Correlation Plot For Numeric or Continous Variables\")\n",
    "\n",
    "#from the below plot,we came to know that both temperature and atemp variables are carrying almost same information\n",
    "#hence there is no need to continue with both variables.\n",
    "#so we need to drop any one of the variables\n",
    "#here I am dropping atemp variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA test for categorical variables\n",
    "\n",
    "for i in cat_cnames:\n",
    "    mod = ols('count' + '~' + i, data = Bike_Data).fit()\n",
    "    aov_table = sm.stats.anova_lm(mod, typ = 2)\n",
    "    print(aov_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the anova result, we are going to drop three variables namely,\n",
    "                            #HOLIDAY\n",
    "                            #WEEKDAY\n",
    "                            #WORKINGDAY\n",
    "            #because these variables having the p-value > 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the variables which has p-value > 0.05 and correlated variable\n",
    "Bike_Data = Bike_Data.drop(['atemp', 'holiday','weekday','workingday'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing variables lets check dimension of the data\n",
    "Bike_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing variables lets check column names of the data\n",
    "Bike_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after removing the variables, we need update numerical and categorical variables\n",
    "\n",
    "# numerical variable\n",
    "cnames = ['temperature','humidity', 'windspeed', 'count']\n",
    "\n",
    "# Categorical variables\n",
    "catnames = ['season', 'year', 'month','weather']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the details of the attributes given, all the numerical variables are normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets visualise the numerical variables to see normality\n",
    "for i in cnames:\n",
    "    print(i)\n",
    "    sm.qqplot(Bike_Data[i])\n",
    "    plt.title(\"Normalized qq plot for \" +i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cnames:\n",
    "    print(i)\n",
    "    sns.distplot(Bike_Data[i],bins='auto',color='blue')\n",
    "    plt.title(\"Distribution plot for \"+i)\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Bike_Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we confirmed the normalized data based on the qqplot,distribution plot and summary of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Required libraries for model development \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Regression problems, we can't pass directly categorical variables.\n",
    "#so we need to convert all categorical variables into dummy variables\n",
    "\n",
    "df = Bike_Data\n",
    "Bike_Data = df\n",
    "\n",
    "#  Converting categorical variables to dummy variables\n",
    "Bike_Data = pd.get_dummies(Bike_Data,columns=catnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bike_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bike_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Divide the data into train and test set \n",
    "\n",
    "X= Bike_Data.drop(['count'],axis=1)\n",
    "y= Bike_Data['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into train and test sets\n",
    "X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Error metrics to calculate the performance of model\n",
    "def MAPE(y_true,y_prediction):\n",
    "    mape= np.mean(np.abs(y_true-y_prediction)/y_true)*100\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Linear Regression model\n",
    "LinearRegression_model= sm.OLS(y_train,X_train).fit()\n",
    "print(LinearRegression_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model prediction on  train data\n",
    "LinearRegression_train= LinearRegression_model.predict(X_train)\n",
    "\n",
    "# Model prediction on test data\n",
    "LinearRegression_test= LinearRegression_model.predict(X_test)\n",
    "\n",
    "# Model performance on train data\n",
    "MAPE_train= MAPE(y_train,LinearRegression_train)\n",
    "\n",
    "# Model performance on test data\n",
    "MAPE_test= MAPE(y_test,LinearRegression_test)\n",
    "\n",
    "# r2 value for train data\n",
    "r2_train= r2_score(y_train,LinearRegression_train)\n",
    "\n",
    "# r2 value for test data-\n",
    "r2_test=r2_score(y_test,LinearRegression_test)\n",
    "\n",
    "# RMSE value for train data\n",
    "RMSE_train = np.sqrt(metrics.mean_squared_error(y_train,LinearRegression_train))\n",
    "\n",
    "# RMSE value for test data\n",
    "RMSE_test = np.sqrt(metrics.mean_squared_error(y_test,LinearRegression_test))\n",
    "\n",
    "print(\"Mean Absolute Precentage Error for train data=\"+str(MAPE_train))\n",
    "print(\"Mean Absolute Precentage Error for test data=\"+str(MAPE_test))\n",
    "print(\"R^2_score for train data=\"+str(r2_train))\n",
    "print(\"R^2_score for test data=\"+str(r2_test))\n",
    "print(\"RMSE for train data=\"+str (RMSE_train))\n",
    "print(\"RMSE for test data=\"+str(RMSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_MetricsLT = {'Model Name': ['Linear Regression'],\n",
    "                 'MAPE_Train':[MAPE_train],\n",
    "                 'MAPE_Test':[MAPE_test],\n",
    "                 'R-squared_Train':[r2_train],\n",
    "                 'R-squared_Test':[r2_test],\n",
    "                 'RMSE_train':[RMSE_train],\n",
    "                 'RMSE_test':[RMSE_test]}\n",
    "\n",
    "LinearRegression_Results = pd.DataFrame(Error_MetricsLT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Build decision tree model on train and test data\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Decision tree for regression\n",
    "DecisionTree_model= DecisionTreeRegressor(max_depth=2).fit(X_train,y_train)\n",
    "\n",
    "# Model prediction on train data\n",
    "DecisionTree_train= DecisionTree_model.predict(X_train)\n",
    "\n",
    "# Model prediction on test data\n",
    "DecisionTree_test= DecisionTree_model.predict(X_test)\n",
    "\n",
    "# Model performance on train data\n",
    "MAPE_train= MAPE(y_train,DecisionTree_train)\n",
    "\n",
    "# Model performance on test data\n",
    "MAPE_test= MAPE(y_test,DecisionTree_test)\n",
    "\n",
    "# r2 value for train data\n",
    "r2_train= r2_score(y_train,DecisionTree_train)\n",
    "\n",
    "# r2 value for test data\n",
    "r2_test=r2_score(y_test,DecisionTree_test)\n",
    "\n",
    "# RMSE value for train data\n",
    "RMSE_train = np.sqrt(metrics.mean_squared_error(y_train,DecisionTree_train))\n",
    "\n",
    "# RMSE value for test data\n",
    "RMSE_test = np.sqrt(metrics.mean_squared_error(y_test,DecisionTree_test))\n",
    "\n",
    "print(\"Mean Absolute Precentage Error for train data=\"+str(MAPE_train))\n",
    "print(\"Mean Absolute Precentage Error for test data=\"+str(MAPE_test))\n",
    "print(\"R^2_score for train data=\"+str(r2_train))\n",
    "print(\"R^2_score for test data=\"+str(r2_test))\n",
    "print(\"RMSE for train data=\"+str(RMSE_train))\n",
    "print(\"RMSE for test data=\"+str(RMSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_MetricsDT = {'Model Name': ['Decision Tree'],\n",
    "                 'MAPE_Train':[MAPE_train],\n",
    "                 'MAPE_Test':[MAPE_test],\n",
    "                 'R-squared_Train':[r2_train],\n",
    "                 'R-squared_Test':[r2_test],\n",
    "                 'RMSE_train':[RMSE_train],\n",
    "                 'RMSE_test':[RMSE_test]}\n",
    "                   \n",
    "DecisionTree_Results = pd.DataFrame(Error_MetricsDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search CV In Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "RandomDecisionTree = DecisionTreeRegressor(random_state = 0)\n",
    "depth = list(range(1,20,2))\n",
    "random_search = {'max_depth': depth}\n",
    "\n",
    "# Lets build a model using above parameters on train data \n",
    "RandomDecisionTree_model= RandomizedSearchCV(RandomDecisionTree,param_distributions= random_search,n_iter=5,cv=5)\n",
    "RandomDecisionTree_model= RandomDecisionTree_model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look into best fit parameters\n",
    "best_parameters = RandomDecisionTree_model.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again rebuild decision tree model using randomsearch best fit parameter ie\n",
    "# with maximum depth = 7\n",
    "RDT_best_model = RandomDecisionTree_model.best_estimator_\n",
    "print(RDT_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on train data \n",
    "RDT_train = RDT_best_model.predict(X_train)\n",
    "\n",
    "# Prediction on test data \n",
    "RDT_test = RDT_best_model.predict(X_test)\n",
    "\n",
    "# Lets check Model performance on both test and train using error metrics of regression like mape,rsquare value\n",
    "# MAPE for train data \n",
    "MAPE_train= MAPE(y_train,RDT_train)\n",
    "\n",
    "# MAPE for test data \n",
    "MAPE_test= MAPE(y_test,RDT_test)\n",
    "\n",
    "# Rsquare for train data\n",
    "r2_train= r2_score(y_train,RDT_train)\n",
    "\n",
    "# Rsquare for test data\n",
    "r2_test=r2_score(y_test,RDT_test)\n",
    "\n",
    "# RMSE value for train data\n",
    "RMSE_train = np.sqrt(metrics.mean_squared_error(y_train,RDT_train))\n",
    "\n",
    "# RMSE value for test data\n",
    "RMSE_test = np.sqrt(metrics.mean_squared_error(y_test,RDT_test))\n",
    "\n",
    "\n",
    "# Lets print the results \n",
    "print(\"Best Parameter=\"+str(best_parameters))\n",
    "print(\"Best Model=\"+str(RDT_best_model))\n",
    "print(\"Mean Absolute Precentage Error for train data=\"+str(MAPE_train))\n",
    "print(\"Mean Absolute Precentage Error for test data=\"+str(MAPE_test))\n",
    "print(\"R^2_score for train data=\"+str(r2_train))\n",
    "print(\"R^2_score for test data=\"+str(r2_test))\n",
    "print(\"RMSE for train data=\"+str (RMSE_train))\n",
    "print(\"RMSE for test data=\"+str(RMSE_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_MetricsRDT = {'Model Name': ['Random Search CV Decision Tree'],\n",
    "                 'MAPE_Train':[MAPE_train],\n",
    "                 'MAPE_Test':[MAPE_test],\n",
    "                 'R-squared_Train':[r2_train],\n",
    "                 'R-squared_Test':[r2_test],\n",
    "                 'RMSE_train':[RMSE_train],\n",
    "                 'RMSE_test':[RMSE_test]}\n",
    "                   \n",
    "RandomDecisionTree_Results = pd.DataFrame(Error_MetricsRDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomDecisionTree_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV in Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "GridDecisionTree= DecisionTreeRegressor(random_state=0)\n",
    "depth= list(range(1,20,2))\n",
    "grid_search= {'max_depth':depth}\n",
    "\n",
    "# Lets build a model using above parameters on train data\n",
    "GridDecisionTree_model= GridSearchCV(GridDecisionTree,param_grid=grid_search,cv=5)\n",
    "GridDecisionTree_model= GridDecisionTree_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look into best fit parameters from gridsearch cv DT\n",
    "best_parameters = GridDecisionTree_model.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again rebuild decision tree model using gridsearch best fit parameter ie\n",
    "# with maximum depth = 7\n",
    "GDT_best_model = GridDecisionTree_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on train data \n",
    "GDT_train = GDT_best_model.predict(X_train)\n",
    "\n",
    "# Prediction on train data  test data-\n",
    "GDT_test = GDT_best_model.predict(X_test)\n",
    "\n",
    "# Lets check Model performance on both test and train using error metrics of regression like mape,rsquare value\n",
    "# MAPE for train data \n",
    "MAPE_train= MAPE(y_train,GDT_train)\n",
    "\n",
    "# MAPE for test data \n",
    "MAPE_test= MAPE(y_test,GDT_test)\n",
    "\n",
    "# Rsquare for train data\n",
    "r2_train= r2_score(y_train,GDT_train)\n",
    "\n",
    "# Rsquare for train data\n",
    "r2_test=r2_score(y_test,GDT_test)\n",
    "\n",
    "# RMSE value for train data\n",
    "RMSE_train = np.sqrt(metrics.mean_squared_error(y_train,GDT_train))\n",
    "\n",
    "# RMSE value for test data\n",
    "RMSE_test = np.sqrt(metrics.mean_squared_error(y_test,GDT_test))\n",
    "\n",
    "\n",
    "print(\"Best Parameter=\"+str(best_parameters))\n",
    "print(\"Best Model=\"+str(GDT_best_model))\n",
    "print(\"Mean Absolute Precentage Error for train data=\"+str(MAPE_train))\n",
    "print(\"Mean Absolute Precentage Error for test data=\"+str(MAPE_test))\n",
    "print(\"R^2_score for train data=\"+str(r2_train))\n",
    "print(\"R^2_score for test data=\"+str(r2_test))\n",
    "print(\"RMSE for train data=\"+str (RMSE_train))\n",
    "print(\"RMSE for test data=\"+str(RMSE_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_MetricsGDT = {'Model Name': ['Grid Search CV Decision Tree'],\n",
    "                 'MAPE_Train':[MAPE_train],\n",
    "                 'MAPE_Test':[MAPE_test],\n",
    "                 'R-squared_Train':[r2_train],\n",
    "                 'R-squared_Test':[r2_test],\n",
    "                 'RMSE_train':[RMSE_train],\n",
    "                 'RMSE_test':[RMSE_test]}\n",
    "                   \n",
    "GridDecisionTree_Results = pd.DataFrame(Error_MetricsGDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridDecisionTree_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraris\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Random Forest for regression\n",
    "RF_model= RandomForestRegressor(n_estimators=100).fit(X_train,y_train)\n",
    "\n",
    "# Prediction on train data\n",
    "RF_train= RF_model.predict(X_train)\n",
    "\n",
    "# Prediction on test data\n",
    "RF_test= RF_model.predict(X_test)\n",
    "\n",
    "# MAPE For train data\n",
    "MAPE_train= MAPE(y_train,RF_train)\n",
    "\n",
    "# MAPE For test data\n",
    "MAPE_test= MAPE(y_test,RF_test)\n",
    "\n",
    "# Rsquare  For train data\n",
    "r2_train= r2_score(y_train,RF_train)\n",
    "\n",
    "# Rsquare  For test data\n",
    "r2_test=r2_score(y_test,RF_test)\n",
    "\n",
    "# RMSE value for train data\n",
    "RMSE_train = np.sqrt(metrics.mean_squared_error(y_train,RF_train))\n",
    "\n",
    "# RMSE value for test data\n",
    "RMSE_test = np.sqrt(metrics.mean_squared_error(y_test,RF_test))\n",
    "\n",
    "print(\"Mean Absolute Precentage Error for train data=\"+str(MAPE_train))\n",
    "print(\"Mean Absolute Precentage Error for test data=\"+str(MAPE_test))\n",
    "print(\"R^2_score for train data=\"+str(r2_train))\n",
    "print(\"R^2_score for test data=\"+str(r2_test))\n",
    "print(\"RMSE for train data=\"+str (RMSE_train))\n",
    "print(\"RMSE for test data=\"+str(RMSE_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_MetricsRF = {'Model Name': ['Random Forest'],\n",
    "                 'MAPE_Train':[MAPE_train],\n",
    "                 'MAPE_Test':[MAPE_test],\n",
    "                 'R-squared_Train':[r2_train],\n",
    "                 'R-squared_Test':[r2_test],\n",
    "                 'RMSE_train':[RMSE_train],\n",
    "                 'RMSE_test':[RMSE_test]}\n",
    "                   \n",
    "RandomForest_Results = pd.DataFrame(Error_MetricsRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search CV in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "RandomRandomForest = RandomForestRegressor(random_state = 0)\n",
    "n_estimator = list(range(1,100,2))\n",
    "depth = list(range(1,20,2))\n",
    "random_search = {'n_estimators':n_estimator, 'max_depth': depth}\n",
    "\n",
    "# Lets build a model using above parameters on train data\n",
    "RandomRandomForest_model= RandomizedSearchCV(RandomRandomForest,param_distributions= random_search,n_iter=5,cv=5)\n",
    "RandomRandomForest_model= RandomRandomForest_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters for model\n",
    "best_parameters = RandomRandomForest_model.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again rebuild random forest  model using gridsearch best fit parameter\n",
    "RRF_best_model = RandomRandomForest_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on train data\n",
    "RRF_train = RRF_best_model.predict(X_train)\n",
    "\n",
    "# Prediction on test data\n",
    "RRF_test = RRF_best_model.predict(X_test)\n",
    "\n",
    "# Lets check Model performance on both test and train using error metrics of regression like mape,rsquare value\n",
    "# MAPE for train data \n",
    "MAPE_train= MAPE(y_train,RRF_train)\n",
    "\n",
    "# MAPE for test data\n",
    "MAPE_test= MAPE(y_test,RRF_test)\n",
    "\n",
    "# Rsquare for train data\n",
    "r2_train= r2_score(y_train,RRF_train)\n",
    "\n",
    "# Rsquare for test data\n",
    "r2_test=r2_score(y_test,RRF_test)\n",
    "\n",
    "# RMSE value for train data\n",
    "RMSE_train = np.sqrt(metrics.mean_squared_error(y_train,RRF_train))\n",
    "\n",
    "# RMSE value for test data\n",
    "RMSE_test = np.sqrt(metrics.mean_squared_error(y_test,RRF_test))\n",
    "\n",
    "\n",
    "print(\"Best Parameter=\"+str(best_parameters))\n",
    "print(\"Best Model=\"+str(RRF_best_model))\n",
    "print(\"Mean Absolute Precentage Error for train data=\"+str(MAPE_train))\n",
    "print(\"Mean Absolute Precentage Error for test data=\"+str(MAPE_test))\n",
    "print(\"R^2_score for train data=\"+str(r2_train))\n",
    "print(\"R^2_score for test data=\"+str(r2_test))\n",
    "print(\"RMSE for train data=\"+str (RMSE_train))\n",
    "print(\"RMSE for test data=\"+str(RMSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_MetricsRSRF = {'Model Name': ['Random Search CV Random Forest'],\n",
    "                 'MAPE_Train':[MAPE_train],\n",
    "                 'MAPE_Test':[MAPE_test],\n",
    "                 'R-squared_Train':[r2_train],\n",
    "                 'R-squared_Test':[r2_test],\n",
    "                 'RMSE_train':[RMSE_train],\n",
    "                 'RMSE_test':[RMSE_test]}\n",
    "                   \n",
    "RandomSearchRandomForest_Results = pd.DataFrame(Error_MetricsRSRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomSearchRandomForest_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search CV in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "GridRandomForest= RandomForestRegressor(random_state=0)\n",
    "n_estimator = list(range(1,20,2))\n",
    "depth= list(range(1,20,2))\n",
    "grid_search= {'n_estimators':n_estimator, 'max_depth': depth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build a model using above parameters on train data using random forest grid search cv \n",
    "GridRandomForest_model= GridSearchCV(GridRandomForest,param_grid=grid_search,cv=5)\n",
    "GridRandomForest_model= GridRandomForest_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best fit parameters for model\n",
    "best_parameters_GRF = GridRandomForest_model.best_params_\n",
    "print(best_parameters_GRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again rebuild random forest model using gridsearch best fit parameter \n",
    "GRF_best_model = GridRandomForest_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on train data\n",
    "GRF_train = GRF_best_model.predict(X_train)\n",
    "\n",
    "# Prediction on test data\n",
    "GRF_test = GRF_best_model.predict(X_test)\n",
    "\n",
    "# Lets check Model performance on both test and train using error metrics of regression like mape,rsquare value\n",
    "# MAPE for train data\n",
    "MAPE_train= MAPE(y_train,GRF_train)\n",
    "\n",
    "# MAPE for test data\n",
    "MAPE_test= MAPE(y_test,GRF_test)\n",
    "\n",
    "# Rsquare for train data\n",
    "r2_train= r2_score(y_train,GRF_train)\n",
    "\n",
    "# Rsquare for test data\n",
    "r2_test=r2_score(y_test,GRF_test)\n",
    "\n",
    "# RMSE value for train data\n",
    "RMSE_train = np.sqrt(metrics.mean_squared_error(y_train,GRF_train))\n",
    "\n",
    "# RMSE value for test data\n",
    "RMSE_test = np.sqrt(metrics.mean_squared_error(y_test,GRF_test))\n",
    "\n",
    "print(\"Best Parameter=\"+str(best_parameters))\n",
    "print(\"Best Model=\"+str(GRF_best_model))\n",
    "print(\"Mean Absolute Precentage Error for train data=\"+str(MAPE_train))\n",
    "print(\"Mean Absolute Precentage Error for test data=\"+str(MAPE_test))\n",
    "print(\"R^2_score for train data=\"+str(r2_train))\n",
    "print(\"R^2_score for test data=\"+str(r2_test))\n",
    "print(\"RMSE for train data=\"+str (RMSE_train))\n",
    "print(\"RMSE for test data=\"+str(RMSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_MetricsGSRF = {'Model Name': ['Grid search CV Random Forest'],\n",
    "                 'MAPE_Train':[MAPE_train],\n",
    "                 'MAPE_Test':[MAPE_test],\n",
    "                 'R-squared_Train':[r2_train],\n",
    "                 'R-squared_Test':[r2_test],\n",
    "                 'RMSE_train':[RMSE_train],\n",
    "                 'RMSE_test':[RMSE_test]}\n",
    "                   \n",
    "GridSearchRandomForest_Results = pd.DataFrame(Error_MetricsGSRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchRandomForest_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Results = pd.concat([LinearRegression_Results,\n",
    "                                DecisionTree_Results,\n",
    "                                RandomDecisionTree_Results,\n",
    "                                GridDecisionTree_Results,\n",
    "                                RandomForest_Results,\n",
    "                                RandomSearchRandomForest_Results,\n",
    "                                GridSearchRandomForest_Results,], ignore_index=True, sort =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above results Random Forest model have optimum values and this\n",
    "# algorithm is good for our data \n",
    "\n",
    "# Lets save the out put of finalized model (RF)\n",
    "\n",
    "input = y_test.reset_index()\n",
    "pred = pd.DataFrame(RF_test,columns = ['pred'])\n",
    "Final_output = pred.join(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_output.to_csv(\"Final_results_py.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
